{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7733466799670977270\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시데이터 모델 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params of multi_AWAC\n",
    "agent_num = 5\n",
    "state_len = 2\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= state_len*agent_num\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 0.3\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-4)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기본함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기본 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예시 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리에서 아래와같이 샘플링됐다고 가정하고 테스트(메모리 아직 안만들어짐)\n",
    "s = np.array([[1,2,1,2,1,2,1,2,1,2,],[2,3,1,2,1,2,3,2,6,7,],[1,4,14,2,1,2,11,2,9,2,]])\n",
    "ns = np.array([[1,29,1,2,10,2,1,29,1,28,],[17,2,16,2,1,21,1,2,1,22,],[1,21,1,22,1,2,7,2,5,2,]])\n",
    "a = np.array([[0],[1],[1],])\n",
    "r = np.array([[2],[4.2],[2.7],])\n",
    "done = np.array([[0],[0],[0],])\n",
    "sample_dataset_by_agent = [s, ns, a, r, done]\n",
    "dataset_list = [sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 개별 critic_qnet agent의 그라디언트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#그래디언트 계산\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        qs = critic_qnet_target_list[0](ns)\n",
    "        sampled_as = get_action(actor_list[0],ns, num_action_samples)\n",
    "        mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "        q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "        # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "        q_val = tf.concat([tf.reshape(tf.gather(critic_qnet_list[0](s)[i],a[i],axis=0),[-1,1]) for i in range(len(s))],axis=0)\n",
    "\n",
    "        loss = loss_fun(q_val, q_target)\n",
    "        # print(loss)\n",
    "\n",
    "        # loss = cal_loss(actor_list[0], critic_qnet_list[0], critic_qnet_target_list[0], s,ns,a,r,done,10,gamma)\n",
    "        # j_loss = joint_loss(actor_list, critic_qnet_list, critic_qnet_target_list,dataset_list,agent_num)\n",
    "        # t.watch(j_loss)\n",
    "        # print([var.name for var in t.watched_variables()])\n",
    "    grads = t.gradient(loss, critic_qnet_list[0].trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, critic_qnet_list[0].trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    #그래디언트 계산\n",
    "    epoch = 100\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            qs = critic_qnet_target_list[0](ns)\n",
    "            sampled_as = get_action(actor_list[0],ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet_list[0](s)[i],a[i],axis=0),[-1,1]) for i in range(len(s))],axis=0)\n",
    "\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            # print(loss)\n",
    "\n",
    "            # loss = cal_loss(actor_list[0], critic_qnet_list[0], critic_qnet_target_list[0], s,ns,a,r,done,10,gamma)\n",
    "            # j_loss = joint_loss(actor_list, critic_qnet_list, critic_qnet_target_list,dataset_list,agent_num)\n",
    "            # t.watch(j_loss)\n",
    "            # print([var.name for var in t.watched_variables()])\n",
    "        grads = t.gradient(loss, critic_qnet_list[0].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, critic_qnet_list[0].trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    #그래디언트 계산\n",
    "    epoch = 100\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            qs = critic_qnet_target_list[0](ns)\n",
    "            sampled_as = get_action(actor_list[0],ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet_list[0](s)[i],a[i],axis=0),[-1,1]) for i in range(len(s))],axis=0)\n",
    "\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            # print(loss)\n",
    "\n",
    "            # loss = cal_loss(actor_list[0], critic_qnet_list[0], critic_qnet_target_list[0], s,ns,a,r,done,10,gamma)\n",
    "            # j_loss = joint_loss(actor_list, critic_qnet_list, critic_qnet_target_list,dataset_list,agent_num)\n",
    "            # t.watch(j_loss)\n",
    "            # print([var.name for var in t.watched_variables()])\n",
    "        grads = t.gradient(loss, critic_qnet_list[0].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, critic_qnet_list[0].trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 에이전트 학습(GPU버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.8 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    epoch = 100\n",
    "    for i in range(100):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            optimizer.apply_gradients(zip(grads[q], critic_qnet_weight_list[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 에이전트 학습(CPU버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.8 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    epoch = 100\n",
    "    for i in range(100):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            optimizer.apply_gradients(zip(grads[q], critic_qnet_weight_list[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 학습 이후 critic_qnet_target 소프트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 2.86 s, total: 27.9 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_list =[]\n",
    "        for j in range(agent_num):\n",
    "            ##데이터 \n",
    "            data = dataset_list[j]\n",
    "            s = data[0]\n",
    "            ns = data[1]\n",
    "            a = data[2]\n",
    "            r = data[3]\n",
    "            done = data[4]\n",
    "            ##에이전트\n",
    "            actor = actor_list[j]\n",
    "            critic_qnet = critic_qnet_list[j]\n",
    "            critic_qnet_target = critic_qnet_target_list[j]\n",
    "            ##로스 계산\n",
    "            qs = critic_qnet_target(ns)\n",
    "            sampled_as = get_action(actor,ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            loss_list.append(loss)\n",
    "        joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "    critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "    critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "    grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "    for q in range(agent_num):\n",
    "        qnet_weights = critic_qnet_weight_list[q]\n",
    "        qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "        grad = grads[q]\n",
    "\n",
    "        optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "\n",
    "        new_target_weights = []\n",
    "        for p, target_weights in enumerate(qnet_target_weights):\n",
    "            qnet_weights_s = qnet_weights[p]\n",
    "            updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "            new_target_weights.append(updated_target_weights_s)\n",
    "        critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss로 target까지 업데이트 후 actor 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30.1 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_list =[]\n",
    "        for j in range(agent_num):\n",
    "            ##데이터 \n",
    "            data = dataset_list[j]\n",
    "            s = data[0]\n",
    "            ns = data[1]\n",
    "            a = data[2]\n",
    "            r = data[3]\n",
    "            done = data[4]\n",
    "            ##에이전트\n",
    "            actor = actor_list[j]\n",
    "            critic_qnet = critic_qnet_list[j]\n",
    "            critic_qnet_target = critic_qnet_target_list[j]\n",
    "            ##로스 계산\n",
    "            qs = critic_qnet_target(ns)\n",
    "            sampled_as = get_action(actor,ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            loss_list.append(loss)\n",
    "        joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "    critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "    critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "    grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "    for q in range(agent_num):\n",
    "        # qnet 업데이트\n",
    "        qnet_weights = critic_qnet_weight_list[q]\n",
    "        qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "        grad = grads[q]\n",
    "\n",
    "        optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "        # target net 업데이트\n",
    "        new_target_weights = []\n",
    "        for p, target_weights in enumerate(qnet_target_weights):\n",
    "            qnet_weights_s = qnet_weights[p]\n",
    "            updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "            new_target_weights.append(updated_target_weights_s)\n",
    "        critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n",
    "        #타겟네트워크까지 업데이트한 후 actor net 업데이트\n",
    "        with tf.GradientTape() as tp:\n",
    "            # log_probability 계산\n",
    "            logits = actor_list[q](s)\n",
    "            m = tfp.distributions.Categorical(logits = logits)\n",
    "            log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "            #가중치항 계산\n",
    "            qs = critic_qnet_target_list[q](s)\n",
    "            action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "            vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "            qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            adv = qas - vs\n",
    "            weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "\n",
    "            #loss\n",
    "            loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "        actor_grad = tp.gradient(loss, actor_list[q].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(actor_grad, actor_list[q].trainable_variables))\n",
    "        # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 33.4 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    epoch = 100\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            # qnet 업데이트\n",
    "            qnet_weights = critic_qnet_weight_list[q]\n",
    "            qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "            grad = grads[q]\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "            # target net 업데이트\n",
    "            new_target_weights = []\n",
    "            for p, target_weights in enumerate(qnet_target_weights):\n",
    "                qnet_weights_s = qnet_weights[p]\n",
    "                updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "                new_target_weights.append(updated_target_weights_s)\n",
    "            critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n",
    "            #타겟네트워크까지 업데이트한 후 actor net 업데이트\n",
    "            with tf.GradientTape() as tp:\n",
    "                # log_probability 계산\n",
    "                logits = actor_list[q](s)\n",
    "                m = tfp.distributions.Categorical(logits = logits)\n",
    "                log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "                #가중치항 계산\n",
    "                qs = critic_qnet_target_list[q](s)\n",
    "                action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "                vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "                qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                adv = qas - vs\n",
    "                weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "\n",
    "                #loss\n",
    "                loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "            actor_grad = tp.gradient(loss, actor_list[q].trainable_variables)\n",
    "            optimizer.apply_gradients(zip(actor_grad, actor_list[q].trainable_variables))\n",
    "            # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.573633, shape=(), dtype=float32)\n",
      "tf.Tensor(9.474024, shape=(), dtype=float32)\n",
      "tf.Tensor(8.261, shape=(), dtype=float32)\n",
      "tf.Tensor(7.4303102, shape=(), dtype=float32)\n",
      "tf.Tensor(6.4824424, shape=(), dtype=float32)\n",
      "tf.Tensor(5.6831493, shape=(), dtype=float32)\n",
      "tf.Tensor(4.940093, shape=(), dtype=float32)\n",
      "tf.Tensor(4.2557425, shape=(), dtype=float32)\n",
      "tf.Tensor(3.7803853, shape=(), dtype=float32)\n",
      "tf.Tensor(3.4873414, shape=(), dtype=float32)\n",
      "tf.Tensor(3.3659835, shape=(), dtype=float32)\n",
      "tf.Tensor(3.363505, shape=(), dtype=float32)\n",
      "tf.Tensor(3.40768, shape=(), dtype=float32)\n",
      "tf.Tensor(3.5388088, shape=(), dtype=float32)\n",
      "tf.Tensor(3.70103, shape=(), dtype=float32)\n",
      "tf.Tensor(4.32361, shape=(), dtype=float32)\n",
      "tf.Tensor(4.663816, shape=(), dtype=float32)\n",
      "tf.Tensor(5.2556596, shape=(), dtype=float32)\n",
      "tf.Tensor(5.432039, shape=(), dtype=float32)\n",
      "tf.Tensor(6.3316603, shape=(), dtype=float32)\n",
      "tf.Tensor(4.8068037, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0011864, shape=(), dtype=float32)\n",
      "tf.Tensor(6.1871448, shape=(), dtype=float32)\n",
      "tf.Tensor(6.7587824, shape=(), dtype=float32)\n",
      "tf.Tensor(8.506586, shape=(), dtype=float32)\n",
      "tf.Tensor(10.633896, shape=(), dtype=float32)\n",
      "tf.Tensor(13.460478, shape=(), dtype=float32)\n",
      "tf.Tensor(16.696468, shape=(), dtype=float32)\n",
      "tf.Tensor(21.138498, shape=(), dtype=float32)\n",
      "tf.Tensor(26.832159, shape=(), dtype=float32)\n",
      "tf.Tensor(34.737465, shape=(), dtype=float32)\n",
      "tf.Tensor(47.071194, shape=(), dtype=float32)\n",
      "tf.Tensor(63.428844, shape=(), dtype=float32)\n",
      "tf.Tensor(88.16732, shape=(), dtype=float32)\n",
      "tf.Tensor(120.68685, shape=(), dtype=float32)\n",
      "tf.Tensor(164.40176, shape=(), dtype=float32)\n",
      "tf.Tensor(211.69962, shape=(), dtype=float32)\n",
      "tf.Tensor(302.247, shape=(), dtype=float32)\n",
      "tf.Tensor(409.6241, shape=(), dtype=float32)\n",
      "tf.Tensor(553.296, shape=(), dtype=float32)\n",
      "tf.Tensor(746.31635, shape=(), dtype=float32)\n",
      "tf.Tensor(1005.14417, shape=(), dtype=float32)\n",
      "tf.Tensor(1352.4677, shape=(), dtype=float32)\n",
      "tf.Tensor(1820.0801, shape=(), dtype=float32)\n",
      "tf.Tensor(2443.4082, shape=(), dtype=float32)\n",
      "tf.Tensor(3269.5627, shape=(), dtype=float32)\n",
      "tf.Tensor(4364.492, shape=(), dtype=float32)\n",
      "tf.Tensor(5805.9917, shape=(), dtype=float32)\n",
      "tf.Tensor(7710.1924, shape=(), dtype=float32)\n",
      "tf.Tensor(10215.51, shape=(), dtype=float32)\n",
      "tf.Tensor(13496.406, shape=(), dtype=float32)\n",
      "tf.Tensor(17805.275, shape=(), dtype=float32)\n",
      "tf.Tensor(23411.033, shape=(), dtype=float32)\n",
      "tf.Tensor(30699.629, shape=(), dtype=float32)\n",
      "tf.Tensor(40183.36, shape=(), dtype=float32)\n",
      "tf.Tensor(52457.188, shape=(), dtype=float32)\n",
      "tf.Tensor(68291.01, shape=(), dtype=float32)\n",
      "tf.Tensor(88718.52, shape=(), dtype=float32)\n",
      "tf.Tensor(114984.92, shape=(), dtype=float32)\n",
      "tf.Tensor(148711.64, shape=(), dtype=float32)\n",
      "tf.Tensor(191953.9, shape=(), dtype=float32)\n",
      "tf.Tensor(247078.66, shape=(), dtype=float32)\n",
      "tf.Tensor(317340.53, shape=(), dtype=float32)\n",
      "tf.Tensor(406775.2, shape=(), dtype=float32)\n",
      "tf.Tensor(520298.1, shape=(), dtype=float32)\n",
      "tf.Tensor(664368.25, shape=(), dtype=float32)\n",
      "tf.Tensor(846595.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1076308.8, shape=(), dtype=float32)\n",
      "tf.Tensor(1365034.9, shape=(), dtype=float32)\n",
      "tf.Tensor(1726574.4, shape=(), dtype=float32)\n",
      "tf.Tensor(2179973.2, shape=(), dtype=float32)\n",
      "tf.Tensor(2746670.5, shape=(), dtype=float32)\n",
      "tf.Tensor(3452904.5, shape=(), dtype=float32)\n",
      "tf.Tensor(4330653.0, shape=(), dtype=float32)\n",
      "tf.Tensor(5418476.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6765615.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8430443.0, shape=(), dtype=float32)\n",
      "tf.Tensor(10483477.0, shape=(), dtype=float32)\n",
      "tf.Tensor(13010243.0, shape=(), dtype=float32)\n",
      "tf.Tensor(16109566.0, shape=(), dtype=float32)\n",
      "tf.Tensor(19903818.0, shape=(), dtype=float32)\n",
      "tf.Tensor(24542996.0, shape=(), dtype=float32)\n",
      "tf.Tensor(30193236.0, shape=(), dtype=float32)\n",
      "tf.Tensor(37068348.0, shape=(), dtype=float32)\n",
      "tf.Tensor(45433880.0, shape=(), dtype=float32)\n",
      "tf.Tensor(55585336.0, shape=(), dtype=float32)\n",
      "tf.Tensor(67875930.0, shape=(), dtype=float32)\n",
      "tf.Tensor(82716750.0, shape=(), dtype=float32)\n",
      "tf.Tensor(100618920.0, shape=(), dtype=float32)\n",
      "tf.Tensor(122194110.0, shape=(), dtype=float32)\n",
      "tf.Tensor(148144820.0, shape=(), dtype=float32)\n",
      "tf.Tensor(179276450.0, shape=(), dtype=float32)\n",
      "tf.Tensor(216553900.0, shape=(), dtype=float32)\n",
      "tf.Tensor(261106690.0, shape=(), dtype=float32)\n",
      "tf.Tensor(314269730.0, shape=(), dtype=float32)\n",
      "tf.Tensor(377575680.0, shape=(), dtype=float32)\n",
      "tf.Tensor(452847000.0, shape=(), dtype=float32)\n",
      "tf.Tensor(542205760.0, shape=(), dtype=float32)\n",
      "tf.Tensor(648065500.0, shape=(), dtype=float32)\n",
      "tf.Tensor(773327900.0, shape=(), dtype=float32)\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    epoch = 100\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "        print(joint_loss)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            # qnet 업데이트\n",
    "            qnet_weights = critic_qnet_weight_list[q]\n",
    "            qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "            grad = grads[q]\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "            # target net 업데이트\n",
    "            new_target_weights = []\n",
    "            for p, target_weights in enumerate(qnet_target_weights):\n",
    "                qnet_weights_s = qnet_weights[p]\n",
    "                updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "                new_target_weights.append(updated_target_weights_s)\n",
    "            critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n",
    "            #타겟네트워크까지 업데이트한 후 actor net 업데이트\n",
    "            with tf.GradientTape() as tp:\n",
    "                # log_probability 계산\n",
    "                logits = actor_list[q](s)\n",
    "                m = tfp.distributions.Categorical(logits = logits)\n",
    "                log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "                #가중치항 계산\n",
    "                qs = critic_qnet_target_list[q](s)\n",
    "                action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "                vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "                qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                adv = qas - vs\n",
    "                weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "\n",
    "                #loss\n",
    "                loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "            actor_grad = tp.gradient(loss, actor_list[q].trainable_variables)\n",
    "            optimizer.apply_gradients(zip(actor_grad, actor_list[q].trainable_variables))\n",
    "            # print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 업데이트된 멀티에이전트 모델 액션얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 1 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 1 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 1 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 1 1], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# logits = actor_list[0](s)\n",
    "# m = tfp.distributions.Categorical(logits = logits)\n",
    "# log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "\n",
    "\n",
    "#데이터\n",
    "for i in range(agent_num):\n",
    "    data = dataset_list[i]\n",
    "    s = data[0]\n",
    "    ns = data[1]\n",
    "    a = data[2]\n",
    "    r = data[3]\n",
    "    done = data[4]\n",
    "    #에이전트\n",
    "    actor = actor_list[i]\n",
    "    # 액션\n",
    "    logits = actor(s)\n",
    "    actions = tf.math.argmax(logits,axis=1,output_type=tf.dtypes.int64,name=None)\n",
    "    print(actions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
