{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 2.3 + cpu + loop op (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5996369439946822371\n",
      "]\n",
      "c:\\Users\\Go\\OneDrive - UOS\\allrepos\\multi_agent_awac\n",
      "['남부', '동화나라', '소리엘', '솜사탕', '예나']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "def reward_co2(co2):\n",
    "    reward = np.exp(-((co2-900)**2)/(2*350**2))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_heat(temp):\n",
    "    reward = np.exp(-((temp-29)**2)/(2*2**2))\n",
    "    return reward\n",
    "\n",
    "## 불러오기 ##\n",
    "path = os.getcwd()\n",
    "site_list = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",\"*\")))\n",
    "site_list = list(map(os.path.basename,site_list))\n",
    "print(path)\n",
    "print(site_list)\n",
    "site = \"솜사탕\"\n",
    "time_mean = \"5T\"\n",
    "csv_by_site_det = []\n",
    "\n",
    "\n",
    "structure = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",site,'*')))\n",
    "structure = list(map(os.path.basename,structure))\n",
    "for j in structure:\n",
    "        temp_data = dd.read_csv(os.path.join(path,\"data\",\"sensing_data\",site,j),encoding='cp949').compute()\n",
    "        temp_data['T/D'] = pd.to_datetime(temp_data['T/D'])\n",
    "       \n",
    "        temp_data.set_index('T/D',inplace=True)\n",
    "        temp_data = temp_data.resample(time_mean,).mean()\n",
    "        temp_data['site_details'] = j\n",
    "        csv_by_site_det.append(temp_data)\n",
    "\n",
    "data = pd.concat(csv_by_site_det)\n",
    "data.columns = ['index','temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']\n",
    "data = data[['temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']]\n",
    "## 라벨링 ##\n",
    "\n",
    "data['vent'] = 0\n",
    "data['people'] = 0\n",
    "data['heat'] = 0\n",
    "\n",
    "\n",
    "data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
    "\n",
    "#0부터 월요일\n",
    "data.loc[data[data.index.dayofweek == 5].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['heat']] = 0\n",
    "\n",
    "data.loc[data[data.index.dayofweek == 6].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['heat']] = 0\n",
    "\n",
    "room_list = ['솜사탕_거실.csv', '솜사탕_방1.csv','솜사탕_방2.csv','솜사탕_방3.csv','솜사탕_방4.csv','솜사탕_부엌.csv',]\n",
    "data_by_room = {}\n",
    "data_room = data.loc[:,['temp','co2','site_details','vent','heat','people']]\n",
    "\n",
    "for i in room_list:\n",
    "    room_name = i\n",
    "    dataset = data_room.loc[data_room.site_details == room_name,:][['temp','co2','vent','heat','people']]\n",
    "\n",
    "    dataset.loc[:,'co2_t+1'] = dataset.loc[:,'co2'].shift(-1)\n",
    "    dataset.loc[:,'temp_t+1'] = dataset.loc[:,'temp'].shift(-1)\n",
    "    dataset.loc[:,'people_t+1'] = dataset.loc[:,'people'].shift(-1)\n",
    "    dataset.loc[:,'reward_co2'] = dataset.loc[:,'co2_t+1'].map(lambda x : reward_co2(x))\n",
    "    dataset.loc[:,'reward_heat'] = dataset.loc[:,'temp_t+1'].map(lambda x : reward_heat(x))\n",
    "    dataset.loc[:,'done'] = False\n",
    "    dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
    "    dataset = dataset.iloc[:-1,:]\n",
    "    #데이터셋 추가\n",
    "    data_by_room[i] = dataset.loc[:,:][['co2','temp','people','co2_t+1','temp_t+1','people_t+1','reward_co2','reward_heat','vent','heat','done',]]\n",
    "\n",
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    \n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_critic_step(s,ns,a,r,done):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    ##로스 계산\n",
    "    qs = critic_qnet_target(ns)\n",
    "    sampled_as = get_action(actor,ns, num_action_samples)\n",
    "    mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "    q_target = r + gamma * mean_qsa * (1 - done)\n",
    "    # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "    q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "    loss = loss_fun(q_val, q_target)\n",
    "  grads = tape.gradient(loss, critic_qnet.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, critic_qnet.trainable_variables))\n",
    "  # target net soft update\n",
    "  soft_g = [(b-a)*tau for a,b in zip(critic_qnet_target.trainable_variables, critic_qnet.trainable_variables)]\n",
    "  # soft_g = (np.array(critic_qnet.trainable_variables) - np.array(critic_qnet_target.trainable_variables))*tau\n",
    "  optimizer.apply_gradients(zip(soft_g, critic_qnet_target.trainable_variables))\n",
    "  \n",
    "  \n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_actor_step(s,a,):\n",
    "  with tf.GradientTape() as tp:\n",
    "      \n",
    "      # log_probability 계산\n",
    "      logits = actor(s)\n",
    "      m = tfp.distributions.Categorical(logits = logits)\n",
    "      log_prob = tf.reshape(m.log_prob(tf.squeeze(a)),[-1,1])\n",
    "      \n",
    "      #가중치항 계산\n",
    "      qs = critic_qnet_target(s)\n",
    "      action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "      vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "      qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "      adv = qas - vs\n",
    "      weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "      \n",
    "      #loss\n",
    "      loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "      \n",
    "  actor_grad = tp.gradient(loss, actor.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "  # train_accuracy.update_state(labels, predictions)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_sync_list = ['솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_방2.csv']\n",
    "\n",
    "s = []\n",
    "ns = []\n",
    "a_list = []\n",
    "r_list = []\n",
    "\n",
    "start_date = '2022-01-04'\n",
    "end_date = '2022-01-27'\n",
    "\n",
    "len_list = []\n",
    "for i in room_sync_list:\n",
    "    temp_data = data_by_room[i].loc[start_date:end_date]\n",
    "    len_notna = len(temp_data.dropna())\n",
    "    len_list.append(len_notna)\n",
    "max_na_room = room_sync_list[np.argmin(len_list)]\n",
    "row_ind = data_by_room[max_na_room].loc[start_date:end_date].dropna().index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_data = data_by_room[i].loc[row_ind]\n",
    "\n",
    "s.append(tf.convert_to_tensor(temp_data.values[:,[0,],],dtype=tf.float32))\n",
    "ns.append(tf.convert_to_tensor(temp_data.values[:,[3,],],dtype=tf.float32))\n",
    "\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[8],],dtype=tf.int64))\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[9],],dtype=tf.int64))\n",
    "\n",
    "\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[6],],dtype=tf.float32))\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[7],],dtype=tf.float32))\n",
    "\n",
    "#정규화 레이어\n",
    "# layer = tf.keras.layers.Normalization(axis=1)\n",
    "# layer.adapt(s[0])\n",
    "\n",
    "#데이터셋 제작\n",
    "# s_dataset = tf.data.Dataset.from_tensor_slices(layer(s[0]))\n",
    "# ns_dataset = tf.data.Dataset.from_tensor_slices(layer(ns[0]))\n",
    "s_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(s[0], axis=0, order=0))\n",
    "ns_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(ns[0], axis=0, order=0))\n",
    "\n",
    "a1_dataset = tf.data.Dataset.from_tensor_slices(a_list[0])\n",
    "a2_dataset = tf.data.Dataset.from_tensor_slices(a_list[1])\n",
    "\n",
    "r1_dataset = tf.data.Dataset.from_tensor_slices(r_list[0])\n",
    "r2_dataset = tf.data.Dataset.from_tensor_slices(r_list[1])\n",
    "\n",
    "done_dataset = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(data_by_room[i].loc[start_date:end_date].values[:,[10]],dtype=tf.float32))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((s_dataset, ns_dataset, ((a1_dataset,r1_dataset),(a2_dataset,r2_dataset),),done_dataset))\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################set params!################\n",
    "# params of multi_AWAC\n",
    "agent_num = 2\n",
    "state_len = 1\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= 1\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 1\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-5)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "############################################\n",
    "# 학습 루프 관련\n",
    "batch_size = 1024\n",
    "buffer_len = len(train_dataset)\n",
    "# n_train = 10\n",
    "n_run = 10\n",
    "\n",
    "#\n",
    "shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "# \n",
    "critic_loss = []\n",
    "actor_loss = []\n",
    "\n",
    "# 초기 모델 생성\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "\n",
    "actor = actor_list[0]\n",
    "critic_qnet = critic_qnet_list[0]\n",
    "critic_qnet_target = critic_qnet_target_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-run- [1]/[10]\n",
      "-run- [2]/[10]\n",
      "-run- [3]/[10]\n",
      "-run- [4]/[10]\n",
      "-run- [5]/[10]\n",
      "-run- [6]/[10]\n",
      "-run- [7]/[10]\n",
      "-run- [8]/[10]\n",
      "-run- [9]/[10]\n",
      "-run- [10]/[10]\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for batch in shuffled_dataset.batch(batch_size).take(1).repeat(n_run):\n",
    "    count += 1\n",
    "    if count % 1 == 0:\n",
    "        print(f\"-run- [{count}]/[{n_run}]\")\n",
    "\n",
    "    s = batch[0]\n",
    "    ns = batch[1]\n",
    "    a = batch[2][0][0]\n",
    "    r = batch[2][0][1]\n",
    "    done = batch[3]\n",
    "\n",
    "    train_critic_step(s,ns,a,r,done)\n",
    "    train_actor_step(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 2.3 + cpu + loop op (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2577165411436842410\n",
      "]\n",
      "c:\\Users\\Go\\OneDrive - UOS\\allrepos\\multi_agent_awac\n",
      "['남부', '동화나라', '소리엘', '솜사탕', '예나']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "def reward_co2(co2):\n",
    "    reward = np.exp(-((co2-900)**2)/(2*350**2))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_heat(temp):\n",
    "    reward = np.exp(-((temp-29)**2)/(2*2**2))\n",
    "    return reward\n",
    "\n",
    "## 불러오기 ##\n",
    "path = os.getcwd()\n",
    "site_list = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",\"*\")))\n",
    "site_list = list(map(os.path.basename,site_list))\n",
    "print(path)\n",
    "print(site_list)\n",
    "site = \"솜사탕\"\n",
    "time_mean = \"5T\"\n",
    "csv_by_site_det = []\n",
    "\n",
    "\n",
    "structure = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",site,'*')))\n",
    "structure = list(map(os.path.basename,structure))\n",
    "for j in structure:\n",
    "        temp_data = dd.read_csv(os.path.join(path,\"data\",\"sensing_data\",site,j),encoding='cp949').compute()\n",
    "        temp_data['T/D'] = pd.to_datetime(temp_data['T/D'])\n",
    "       \n",
    "        temp_data.set_index('T/D',inplace=True)\n",
    "        temp_data = temp_data.resample(time_mean,).mean()\n",
    "        temp_data['site_details'] = j\n",
    "        csv_by_site_det.append(temp_data)\n",
    "\n",
    "data = pd.concat(csv_by_site_det)\n",
    "data.columns = ['index','temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']\n",
    "data = data[['temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']]\n",
    "## 라벨링 ##\n",
    "\n",
    "data['vent'] = 0\n",
    "data['people'] = 0\n",
    "data['heat'] = 0\n",
    "\n",
    "\n",
    "data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
    "\n",
    "#0부터 월요일\n",
    "data.loc[data[data.index.dayofweek == 5].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['heat']] = 0\n",
    "\n",
    "data.loc[data[data.index.dayofweek == 6].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['heat']] = 0\n",
    "\n",
    "room_list = ['솜사탕_거실.csv', '솜사탕_방1.csv','솜사탕_방2.csv','솜사탕_방3.csv','솜사탕_방4.csv','솜사탕_부엌.csv',]\n",
    "data_by_room = {}\n",
    "data_room = data.loc[:,['temp','co2','site_details','vent','heat','people']]\n",
    "\n",
    "for i in room_list:\n",
    "    room_name = i\n",
    "    dataset = data_room.loc[data_room.site_details == room_name,:][['temp','co2','vent','heat','people']]\n",
    "\n",
    "    dataset.loc[:,'co2_t+1'] = dataset.loc[:,'co2'].shift(-1)\n",
    "    dataset.loc[:,'temp_t+1'] = dataset.loc[:,'temp'].shift(-1)\n",
    "    dataset.loc[:,'people_t+1'] = dataset.loc[:,'people'].shift(-1)\n",
    "    dataset.loc[:,'reward_co2'] = dataset.loc[:,'co2_t+1'].map(lambda x : reward_co2(x))\n",
    "    dataset.loc[:,'reward_heat'] = dataset.loc[:,'temp_t+1'].map(lambda x : reward_heat(x))\n",
    "    dataset.loc[:,'done'] = False\n",
    "    dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
    "    dataset = dataset.iloc[:-1,:]\n",
    "    #데이터셋 추가\n",
    "    data_by_room[i] = dataset.loc[:,:][['co2','temp','people','co2_t+1','temp_t+1','people_t+1','reward_co2','reward_heat','vent','heat','done',]]\n",
    "\n",
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    \n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic_step(s,ns,a,r,done):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    ##로스 계산\n",
    "    qs = critic_qnet_target(ns)\n",
    "    sampled_as = get_action(actor,ns, num_action_samples)\n",
    "    mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "    q_target = r + gamma * mean_qsa * (1 - done)\n",
    "    # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "    q_val = tf.reshape(tf.gather_nd(critic_qnet(s),a,batch_dims=1,),[-1,1])\n",
    "    loss = loss_fun(q_val, q_target)\n",
    "  grads = tape.gradient(loss, critic_qnet.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, critic_qnet.trainable_variables))\n",
    "  # target net soft update\n",
    "  soft_g = [(b-a)*tau for a,b in zip(critic_qnet_target.trainable_variables, critic_qnet.trainable_variables)]\n",
    "  # soft_g = (np.array(critic_qnet.trainable_variables) - np.array(critic_qnet_target.trainable_variables))*tau\n",
    "  optimizer.apply_gradients(zip(soft_g, critic_qnet_target.trainable_variables))\n",
    "  \n",
    "  \n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_actor_step(s,a,):\n",
    "  with tf.GradientTape() as tp:\n",
    "      \n",
    "      # log_probability 계산\n",
    "      logits = actor(s)\n",
    "      m = tfp.distributions.Categorical(logits = logits)\n",
    "      log_prob = tf.reshape(m.log_prob(tf.squeeze(a)),[-1,1])\n",
    "      \n",
    "      #가중치항 계산\n",
    "      qs = critic_qnet_target(s)\n",
    "      action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "      vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "      qas = tf.reshape(tf.gather_nd(qs,a,batch_dims=1,),[-1,1])\n",
    "      adv = qas - vs\n",
    "      weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "      \n",
    "      #loss\n",
    "      loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "      \n",
    "  actor_grad = tp.gradient(loss, actor.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "  # train_accuracy.update_state(labels, predictions)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_sync_list = ['솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_방2.csv']\n",
    "\n",
    "s = []\n",
    "ns = []\n",
    "a_list = []\n",
    "r_list = []\n",
    "\n",
    "start_date = '2022-01-04'\n",
    "end_date = '2022-01-27'\n",
    "\n",
    "len_list = []\n",
    "for i in room_sync_list:\n",
    "    temp_data = data_by_room[i].loc[start_date:end_date]\n",
    "    len_notna = len(temp_data.dropna())\n",
    "    len_list.append(len_notna)\n",
    "max_na_room = room_sync_list[np.argmin(len_list)]\n",
    "row_ind = data_by_room[max_na_room].loc[start_date:end_date].dropna().index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_data = data_by_room[i].loc[row_ind]\n",
    "\n",
    "s.append(tf.convert_to_tensor(temp_data.values[:,[0,],],dtype=tf.float32))\n",
    "ns.append(tf.convert_to_tensor(temp_data.values[:,[3,],],dtype=tf.float32))\n",
    "\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[8],],dtype=tf.int64))\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[9],],dtype=tf.int64))\n",
    "\n",
    "\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[6],],dtype=tf.float32))\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[7],],dtype=tf.float32))\n",
    "\n",
    "#정규화 레이어\n",
    "# layer = tf.keras.layers.Normalization(axis=1)\n",
    "# layer.adapt(s[0])\n",
    "\n",
    "#데이터셋 제작\n",
    "# s_dataset = tf.data.Dataset.from_tensor_slices(layer(s[0]))\n",
    "# ns_dataset = tf.data.Dataset.from_tensor_slices(layer(ns[0]))\n",
    "s_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(s[0], axis=0, order=0))\n",
    "ns_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(ns[0], axis=0, order=0))\n",
    "\n",
    "a1_dataset = tf.data.Dataset.from_tensor_slices(a_list[0])\n",
    "a2_dataset = tf.data.Dataset.from_tensor_slices(a_list[1])\n",
    "\n",
    "r1_dataset = tf.data.Dataset.from_tensor_slices(r_list[0])\n",
    "r2_dataset = tf.data.Dataset.from_tensor_slices(r_list[1])\n",
    "\n",
    "done_dataset = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(data_by_room[i].loc[start_date:end_date].values[:,[10]],dtype=tf.float32))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((s_dataset, ns_dataset, ((a1_dataset,r1_dataset),(a2_dataset,r2_dataset),),done_dataset))\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################set params!################\n",
    "# params of multi_AWAC\n",
    "agent_num = 2\n",
    "state_len = 1\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= 1\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 1\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-5)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "############################################\n",
    "# 학습 루프 관련\n",
    "batch_size = 1024\n",
    "buffer_len = len(train_dataset)\n",
    "# n_train = 10\n",
    "n_run = 10\n",
    "\n",
    "#\n",
    "shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "# \n",
    "critic_loss = []\n",
    "actor_loss = []\n",
    "\n",
    "# 초기 모델 생성\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "\n",
    "actor = actor_list[0]\n",
    "critic_qnet = critic_qnet_list[0]\n",
    "critic_qnet_target = critic_qnet_target_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-run- [1]/[10]\n",
      "-run- [2]/[10]\n",
      "-run- [3]/[10]\n",
      "-run- [4]/[10]\n",
      "-run- [5]/[10]\n",
      "-run- [6]/[10]\n",
      "-run- [7]/[10]\n",
      "-run- [8]/[10]\n",
      "-run- [9]/[10]\n",
      "-run- [10]/[10]\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for batch in shuffled_dataset.batch(batch_size).take(1).repeat(n_run):\n",
    "    count += 1\n",
    "    if count % 1 == 0:\n",
    "        print(f\"-run- [{count}]/[{n_run}]\")\n",
    "\n",
    "    s = batch[0]\n",
    "    ns = batch[1]\n",
    "    a = batch[2][0][0]\n",
    "    r = batch[2][0][1]\n",
    "    done = batch[3]\n",
    "\n",
    "    train_critic_step(s,ns,a,r,done)\n",
    "    train_actor_step(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 2.8 + GPU + loop op (O) + XLA (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6712686120815539418\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6568774921938282991\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "c:\\Users\\Go\\OneDrive - UOS\\allrepos\\multi_agent_awac\n",
      "['남부', '동화나라', '소리엘', '솜사탕', '예나']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:61: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:62: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:63: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_24412\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "def reward_co2(co2):\n",
    "    reward = np.exp(-((co2-900)**2)/(2*350**2))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_heat(temp):\n",
    "    reward = np.exp(-((temp-29)**2)/(2*2**2))\n",
    "    return reward\n",
    "\n",
    "## 불러오기 ##\n",
    "path = os.getcwd()\n",
    "site_list = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",\"*\")))\n",
    "site_list = list(map(os.path.basename,site_list))\n",
    "print(path)\n",
    "print(site_list)\n",
    "site = \"솜사탕\"\n",
    "time_mean = \"5T\"\n",
    "csv_by_site_det = []\n",
    "\n",
    "\n",
    "structure = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",site,'*')))\n",
    "structure = list(map(os.path.basename,structure))\n",
    "for j in structure:\n",
    "        temp_data = dd.read_csv(os.path.join(path,\"data\",\"sensing_data\",site,j),encoding='cp949').compute()\n",
    "        temp_data['T/D'] = pd.to_datetime(temp_data['T/D'])\n",
    "       \n",
    "        temp_data.set_index('T/D',inplace=True)\n",
    "        temp_data = temp_data.resample(time_mean,).mean()\n",
    "        temp_data['site_details'] = j\n",
    "        csv_by_site_det.append(temp_data)\n",
    "\n",
    "data = pd.concat(csv_by_site_det)\n",
    "data.columns = ['index','temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']\n",
    "data = data[['temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']]\n",
    "## 라벨링 ##\n",
    "\n",
    "data['vent'] = 0\n",
    "data['people'] = 0\n",
    "data['heat'] = 0\n",
    "\n",
    "\n",
    "data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
    "\n",
    "#0부터 월요일\n",
    "data.loc[data[data.index.dayofweek == 5].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['heat']] = 0\n",
    "\n",
    "data.loc[data[data.index.dayofweek == 6].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['heat']] = 0\n",
    "\n",
    "room_list = ['솜사탕_거실.csv', '솜사탕_방1.csv','솜사탕_방2.csv','솜사탕_방3.csv','솜사탕_방4.csv','솜사탕_부엌.csv',]\n",
    "data_by_room = {}\n",
    "data_room = data.loc[:,['temp','co2','site_details','vent','heat','people']]\n",
    "\n",
    "for i in room_list:\n",
    "    room_name = i\n",
    "    dataset = data_room.loc[data_room.site_details == room_name,:][['temp','co2','vent','heat','people']]\n",
    "\n",
    "    dataset.loc[:,'co2_t+1'] = dataset.loc[:,'co2'].shift(-1)\n",
    "    dataset.loc[:,'temp_t+1'] = dataset.loc[:,'temp'].shift(-1)\n",
    "    dataset.loc[:,'people_t+1'] = dataset.loc[:,'people'].shift(-1)\n",
    "    dataset.loc[:,'reward_co2'] = dataset.loc[:,'co2_t+1'].map(lambda x : reward_co2(x))\n",
    "    dataset.loc[:,'reward_heat'] = dataset.loc[:,'temp_t+1'].map(lambda x : reward_heat(x))\n",
    "    dataset.loc[:,'done'] = False\n",
    "    dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
    "    dataset = dataset.iloc[:-1,:]\n",
    "    #데이터셋 추가\n",
    "    data_by_room[i] = dataset.loc[:,:][['co2','temp','people','co2_t+1','temp_t+1','people_t+1','reward_co2','reward_heat','vent','heat','done',]]\n",
    "\n",
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    \n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_critic_step(s,ns,a,r,done):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    ##로스 계산\n",
    "    qs = critic_qnet_target(ns)\n",
    "    sampled_as = get_action(actor,ns, num_action_samples)\n",
    "    mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "    q_target = r + gamma * mean_qsa * (1 - done)\n",
    "    # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "    q_val = tf.reshape(tf.gather_nd(critic_qnet(s),a,batch_dims=1,),[-1,1])\n",
    "    loss = loss_fun(q_val, q_target)\n",
    "  grads = tape.gradient(loss, critic_qnet.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, critic_qnet.trainable_variables))\n",
    "  # target net soft update\n",
    "  soft_g = [(b-a)*tau for a,b in zip(critic_qnet_target.trainable_variables, critic_qnet.trainable_variables)]\n",
    "  # soft_g = (np.array(critic_qnet.trainable_variables) - np.array(critic_qnet_target.trainable_variables))*tau\n",
    "  optimizer.apply_gradients(zip(soft_g, critic_qnet_target.trainable_variables))\n",
    "  \n",
    "  \n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_actor_step(s,a,):\n",
    "  with tf.GradientTape() as tp:\n",
    "      \n",
    "      # log_probability 계산\n",
    "      logits = actor(s)\n",
    "      m = tfp.distributions.Categorical(logits = logits)\n",
    "      log_prob = tf.reshape(m.log_prob(tf.squeeze(a)),[-1,1])\n",
    "      \n",
    "      #가중치항 계산\n",
    "      qs = critic_qnet_target(s)\n",
    "      action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "      vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "      qas = tf.reshape(tf.gather_nd(qs,a,batch_dims=1,),[-1,1])\n",
    "      adv = qas - vs\n",
    "      weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "      \n",
    "      #loss\n",
    "      loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "      \n",
    "  actor_grad = tp.gradient(loss, actor.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "  # train_accuracy.update_state(labels, predictions)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_sync_list = ['솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_방2.csv']\n",
    "\n",
    "s = []\n",
    "ns = []\n",
    "a_list = []\n",
    "r_list = []\n",
    "\n",
    "start_date = '2022-01-04'\n",
    "end_date = '2022-01-27'\n",
    "\n",
    "len_list = []\n",
    "for i in room_sync_list:\n",
    "    temp_data = data_by_room[i].loc[start_date:end_date]\n",
    "    len_notna = len(temp_data.dropna())\n",
    "    len_list.append(len_notna)\n",
    "max_na_room = room_sync_list[np.argmin(len_list)]\n",
    "row_ind = data_by_room[max_na_room].loc[start_date:end_date].dropna().index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_data = data_by_room[i].loc[row_ind]\n",
    "\n",
    "s.append(tf.convert_to_tensor(temp_data.values[:,[0,],],dtype=tf.float32))\n",
    "ns.append(tf.convert_to_tensor(temp_data.values[:,[3,],],dtype=tf.float32))\n",
    "\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[8],],dtype=tf.int64))\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[9],],dtype=tf.int64))\n",
    "\n",
    "\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[6],],dtype=tf.float32))\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[7],],dtype=tf.float32))\n",
    "\n",
    "#정규화 레이어\n",
    "# layer = tf.keras.layers.Normalization(axis=1)\n",
    "# layer.adapt(s[0])\n",
    "\n",
    "#데이터셋 제작\n",
    "# s_dataset = tf.data.Dataset.from_tensor_slices(layer(s[0]))\n",
    "# ns_dataset = tf.data.Dataset.from_tensor_slices(layer(ns[0]))\n",
    "s_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(s[0], axis=0, order=0))\n",
    "ns_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(ns[0], axis=0, order=0))\n",
    "\n",
    "a1_dataset = tf.data.Dataset.from_tensor_slices(a_list[0])\n",
    "a2_dataset = tf.data.Dataset.from_tensor_slices(a_list[1])\n",
    "\n",
    "r1_dataset = tf.data.Dataset.from_tensor_slices(r_list[0])\n",
    "r2_dataset = tf.data.Dataset.from_tensor_slices(r_list[1])\n",
    "\n",
    "done_dataset = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(data_by_room[i].loc[start_date:end_date].values[:,[10]],dtype=tf.float32))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((s_dataset, ns_dataset, ((a1_dataset,r1_dataset),(a2_dataset,r2_dataset),),done_dataset))\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################set params!################\n",
    "# params of multi_AWAC\n",
    "agent_num = 2\n",
    "state_len = 1\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= 1\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 1\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-5)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "############################################\n",
    "# 학습 루프 관련\n",
    "batch_size = 1024\n",
    "buffer_len = len(train_dataset)\n",
    "# n_train = 10\n",
    "n_run = 10\n",
    "\n",
    "#\n",
    "shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "# \n",
    "critic_loss = []\n",
    "actor_loss = []\n",
    "\n",
    "# 초기 모델 생성\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "\n",
    "actor = actor_list[0]\n",
    "critic_qnet = critic_qnet_list[0]\n",
    "critic_qnet_target = critic_qnet_target_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-run- [1]/[10]\n",
      "-run- [2]/[10]\n",
      "-run- [3]/[10]\n",
      "-run- [4]/[10]\n",
      "-run- [5]/[10]\n",
      "-run- [6]/[10]\n",
      "-run- [7]/[10]\n",
      "-run- [8]/[10]\n",
      "-run- [9]/[10]\n",
      "-run- [10]/[10]\n",
      "CPU times: total: 24.5 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for batch in shuffled_dataset.batch(batch_size).take(1).repeat(n_run):\n",
    "    count += 1\n",
    "    if count % 1 == 0:\n",
    "        print(f\"-run- [{count}]/[{n_run}]\")\n",
    "\n",
    "    s = batch[0]\n",
    "    ns = batch[1]\n",
    "    a = batch[2][0][0]\n",
    "    r = batch[2][0][1]\n",
    "    done = batch[3]\n",
    "\n",
    "    train_critic_step(s,ns,a,r,done)\n",
    "    train_actor_step(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 2.8 + gpu + loop op (O) + XLA (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10254412107032212236\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9458029629686685025\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "c:\\Users\\Go\\OneDrive - UOS\\allrepos\\multi_agent_awac\n",
      "['남부', '동화나라', '소리엘', '솜사탕', '예나']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:61: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:62: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:63: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
      "C:\\Users\\Go\\AppData\\Local\\Temp\\ipykernel_7904\\2697143665.py:88: FutureWarning: `include_start` and `include_end` are deprecated in favour of `inclusive`.\n",
      "  dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "def reward_co2(co2):\n",
    "    reward = np.exp(-((co2-900)**2)/(2*350**2))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_heat(temp):\n",
    "    reward = np.exp(-((temp-29)**2)/(2*2**2))\n",
    "    return reward\n",
    "\n",
    "## 불러오기 ##\n",
    "path = os.getcwd()\n",
    "site_list = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",\"*\")))\n",
    "site_list = list(map(os.path.basename,site_list))\n",
    "print(path)\n",
    "print(site_list)\n",
    "site = \"솜사탕\"\n",
    "time_mean = \"5T\"\n",
    "csv_by_site_det = []\n",
    "\n",
    "\n",
    "structure = sorted(glob(os.path.join(path,\"data\",\"sensing_data\",site,'*')))\n",
    "structure = list(map(os.path.basename,structure))\n",
    "for j in structure:\n",
    "        temp_data = dd.read_csv(os.path.join(path,\"data\",\"sensing_data\",site,j),encoding='cp949').compute()\n",
    "        temp_data['T/D'] = pd.to_datetime(temp_data['T/D'])\n",
    "       \n",
    "        temp_data.set_index('T/D',inplace=True)\n",
    "        temp_data = temp_data.resample(time_mean,).mean()\n",
    "        temp_data['site_details'] = j\n",
    "        csv_by_site_det.append(temp_data)\n",
    "\n",
    "data = pd.concat(csv_by_site_det)\n",
    "data.columns = ['index','temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']\n",
    "data = data[['temp','hum','pm1','pm2.5','pm4','pm10','co2','tvoc','site_details']]\n",
    "## 라벨링 ##\n",
    "\n",
    "data['vent'] = 0\n",
    "data['people'] = 0\n",
    "data['heat'] = 0\n",
    "\n",
    "\n",
    "data.loc[data.between_time('12:00:00', '18:00:00',include_end=False).index,['vent']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['people']] = 1\n",
    "data.loc[data.between_time('9:00:00', '16:00:00',include_end=False).index,['heat']] = 1\n",
    "\n",
    "#0부터 월요일\n",
    "data.loc[data[data.index.dayofweek == 5].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 5].index,['heat']] = 0\n",
    "\n",
    "data.loc[data[data.index.dayofweek == 6].index,['vent']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['people']] = 0\n",
    "data.loc[data[data.index.dayofweek == 6].index,['heat']] = 0\n",
    "\n",
    "room_list = ['솜사탕_거실.csv', '솜사탕_방1.csv','솜사탕_방2.csv','솜사탕_방3.csv','솜사탕_방4.csv','솜사탕_부엌.csv',]\n",
    "data_by_room = {}\n",
    "data_room = data.loc[:,['temp','co2','site_details','vent','heat','people']]\n",
    "\n",
    "for i in room_list:\n",
    "    room_name = i\n",
    "    dataset = data_room.loc[data_room.site_details == room_name,:][['temp','co2','vent','heat','people']]\n",
    "\n",
    "    dataset.loc[:,'co2_t+1'] = dataset.loc[:,'co2'].shift(-1)\n",
    "    dataset.loc[:,'temp_t+1'] = dataset.loc[:,'temp'].shift(-1)\n",
    "    dataset.loc[:,'people_t+1'] = dataset.loc[:,'people'].shift(-1)\n",
    "    dataset.loc[:,'reward_co2'] = dataset.loc[:,'co2_t+1'].map(lambda x : reward_co2(x))\n",
    "    dataset.loc[:,'reward_heat'] = dataset.loc[:,'temp_t+1'].map(lambda x : reward_heat(x))\n",
    "    dataset.loc[:,'done'] = False\n",
    "    dataset.loc[dataset.between_time('00:00:00', '00:05:00',include_end=False).index,['done']] = True\n",
    "    dataset = dataset.iloc[:-1,:]\n",
    "    #데이터셋 추가\n",
    "    data_by_room[i] = dataset.loc[:,:][['co2','temp','people','co2_t+1','temp_t+1','people_t+1','reward_co2','reward_heat','vent','heat','done',]]\n",
    "\n",
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    \n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_critic_step(s,ns,a,r,done):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    ##로스 계산\n",
    "    qs = critic_qnet_target(ns)\n",
    "    sampled_as = get_action(actor,ns, num_action_samples)\n",
    "    mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "    q_target = r + gamma * mean_qsa * (1 - done)\n",
    "    # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "    q_val = tf.reshape(tf.gather_nd(critic_qnet(s),a,batch_dims=1,),[-1,1])\n",
    "    loss = loss_fun(q_val, q_target)\n",
    "  grads = tape.gradient(loss, critic_qnet.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, critic_qnet.trainable_variables))\n",
    "  # target net soft update\n",
    "  soft_g = [(b-a)*tau for a,b in zip(critic_qnet_target.trainable_variables, critic_qnet.trainable_variables)]\n",
    "  # soft_g = (np.array(critic_qnet.trainable_variables) - np.array(critic_qnet_target.trainable_variables))*tau\n",
    "  optimizer.apply_gradients(zip(soft_g, critic_qnet_target.trainable_variables))\n",
    "  \n",
    "  \n",
    "  return loss\n",
    "\n",
    "@tf.function()\n",
    "def train_actor_step(s,a,):\n",
    "  with tf.GradientTape() as tp:\n",
    "      \n",
    "      # log_probability 계산\n",
    "      logits = actor(s)\n",
    "      m = tfp.distributions.Categorical(logits = logits)\n",
    "      log_prob = tf.reshape(m.log_prob(tf.squeeze(a)),[-1,1])\n",
    "      \n",
    "      #가중치항 계산\n",
    "      qs = critic_qnet_target(s)\n",
    "      action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "      vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "      qas = tf.reshape(tf.gather_nd(qs,a,batch_dims=1,),[-1,1])\n",
    "      adv = qas - vs\n",
    "      weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "      \n",
    "      #loss\n",
    "      loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "      \n",
    "  actor_grad = tp.gradient(loss, actor.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "  # train_accuracy.update_state(labels, predictions)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_sync_list = ['솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv', '솜사탕_거실.csv']\n",
    "# room_sync_list = ['솜사탕_거실.csv', '솜사탕_방2.csv']\n",
    "\n",
    "s = []\n",
    "ns = []\n",
    "a_list = []\n",
    "r_list = []\n",
    "\n",
    "start_date = '2022-01-04'\n",
    "end_date = '2022-01-27'\n",
    "\n",
    "len_list = []\n",
    "for i in room_sync_list:\n",
    "    temp_data = data_by_room[i].loc[start_date:end_date]\n",
    "    len_notna = len(temp_data.dropna())\n",
    "    len_list.append(len_notna)\n",
    "max_na_room = room_sync_list[np.argmin(len_list)]\n",
    "row_ind = data_by_room[max_na_room].loc[start_date:end_date].dropna().index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_data = data_by_room[i].loc[row_ind]\n",
    "\n",
    "s.append(tf.convert_to_tensor(temp_data.values[:,[0,],],dtype=tf.float32))\n",
    "ns.append(tf.convert_to_tensor(temp_data.values[:,[3,],],dtype=tf.float32))\n",
    "\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[8],],dtype=tf.int64))\n",
    "a_list.append(tf.convert_to_tensor(temp_data.values[:,[9],],dtype=tf.int64))\n",
    "\n",
    "\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[6],],dtype=tf.float32))\n",
    "r_list.append(tf.convert_to_tensor(temp_data.values[:,[7],],dtype=tf.float32))\n",
    "\n",
    "#정규화 레이어\n",
    "# layer = tf.keras.layers.Normalization(axis=1)\n",
    "# layer.adapt(s[0])\n",
    "\n",
    "#데이터셋 제작\n",
    "# s_dataset = tf.data.Dataset.from_tensor_slices(layer(s[0]))\n",
    "# ns_dataset = tf.data.Dataset.from_tensor_slices(layer(ns[0]))\n",
    "s_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(s[0], axis=0, order=0))\n",
    "ns_dataset = tf.data.Dataset.from_tensor_slices(tf.keras.utils.normalize(ns[0], axis=0, order=0))\n",
    "\n",
    "a1_dataset = tf.data.Dataset.from_tensor_slices(a_list[0])\n",
    "a2_dataset = tf.data.Dataset.from_tensor_slices(a_list[1])\n",
    "\n",
    "r1_dataset = tf.data.Dataset.from_tensor_slices(r_list[0])\n",
    "r2_dataset = tf.data.Dataset.from_tensor_slices(r_list[1])\n",
    "\n",
    "done_dataset = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(data_by_room[i].loc[start_date:end_date].values[:,[10]],dtype=tf.float32))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((s_dataset, ns_dataset, ((a1_dataset,r1_dataset),(a2_dataset,r2_dataset),),done_dataset))\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################set params!################\n",
    "# params of multi_AWAC\n",
    "agent_num = 2\n",
    "state_len = 1\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= 1\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 1\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-5)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "############################################\n",
    "# 학습 루프 관련\n",
    "batch_size = 1024\n",
    "buffer_len = len(train_dataset)\n",
    "# n_train = 10\n",
    "n_run = 10\n",
    "\n",
    "#\n",
    "shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "# \n",
    "critic_loss = []\n",
    "actor_loss = []\n",
    "\n",
    "# 초기 모델 생성\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "\n",
    "actor = actor_list[0]\n",
    "critic_qnet = critic_qnet_list[0]\n",
    "critic_qnet_target = critic_qnet_target_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-run- [1]/[10]\n",
      "-run- [2]/[10]\n",
      "-run- [3]/[10]\n",
      "-run- [4]/[10]\n",
      "-run- [5]/[10]\n",
      "-run- [6]/[10]\n",
      "-run- [7]/[10]\n",
      "-run- [8]/[10]\n",
      "-run- [9]/[10]\n",
      "-run- [10]/[10]\n",
      "CPU times: total: 1.59 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for batch in shuffled_dataset.batch(batch_size).take(1).repeat(n_run):\n",
    "    count += 1\n",
    "    if count % 1 == 0:\n",
    "        print(f\"-run- [{count}]/[{n_run}]\")\n",
    "\n",
    "    s = batch[0]\n",
    "    ns = batch[1]\n",
    "    a = batch[2][0][0]\n",
    "    r = batch[2][0][1]\n",
    "    done = batch[3]\n",
    "\n",
    "    train_critic_step(s,ns,a,r,done)\n",
    "    train_actor_step(s,a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pro1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
