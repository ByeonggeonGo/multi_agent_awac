{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.client import device_lib\n",
    "from matplotlib import font_manager, rc, dates\n",
    "font_path = \"NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params of multi_AWAC\n",
    "agent_num = 5\n",
    "state_len = 2\n",
    "hidden_structure= [64, 128, 256, 128, 64]\n",
    "input_shape= state_len*agent_num\n",
    "output_shape= 2\n",
    "act_function= 'relu'\n",
    "lam = 0.3\n",
    "optimizer= keras.optimizers.Adam(learning_rate=3 * 1e-4)\n",
    "gamma= 0.9\n",
    "tau = 0.1\n",
    "num_action_samples = 8\n",
    "loss_fun = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기본함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense model 만드는 함수\n",
    "def make_dense_model(hidden_structure,input_shape,act_function,output_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    for i, val in enumerate(hidden_structure):\n",
    "        n_percep = val \n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "        elif  i != 0 and i != len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "        elif  i == len(hidden_structure)-1:\n",
    "            x = keras.layers.Dense(n_percep,activation= act_function)(x)\n",
    "    x = keras.layers.Dense(output_shape)(x)\n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return dense_model\n",
    "\n",
    "# multi_agent structure 만드는 함수\n",
    "def build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape):\n",
    "    actor_list = []\n",
    "    critic_qnet_list = []\n",
    "    critic_qnet_target_list = []\n",
    "    for i in range(agent_num):\n",
    "        actor_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "        critic_qnet_target_list.append(make_dense_model(hidden_structure,input_shape,act_function,output_shape))\n",
    "    return actor_list, critic_qnet_list, critic_qnet_target_list\n",
    "\n",
    "\n",
    "def get_action(actor,state, num_samples: int = 3):\n",
    "    logit_sam = actor(state)\n",
    "    m = tfp.distributions.Categorical(logits = logit_sam)\n",
    "    return tf.reshape(m.sample(num_samples),[-1,num_samples])\n",
    "\n",
    "def get_mean_qsa(qs,sampled_as):\n",
    "    mean_q = tf.concat([tf.reshape(tf.gather(qs[i],sampled_as[i], axis=0),[1,-1]) for i in range(len(qs))],axis=0)\n",
    "    mean_q = tf.math.reduce_mean(mean_q,axis=1,keepdims=True)\n",
    "    return mean_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기본 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예시 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리에서 아래와같이 샘플링됐다고 가정하고 테스트(메모리 아직 안만들어짐)\n",
    "s = np.array([[1,2,1,2,1,2,1,2,1,2,],[2,3,1,2,1,2,3,2,6,7,],[1,4,14,2,1,2,11,2,9,2,]])\n",
    "ns = np.array([[1,29,1,2,10,2,1,29,1,28,],[17,2,16,2,1,21,1,2,1,22,],[1,21,1,22,1,2,7,2,5,2,]])\n",
    "a = np.array([[0],[1],[1],])\n",
    "r = np.array([[2],[4.2],[2.7],])\n",
    "done = np.array([[0],[0],[0],])\n",
    "sample_dataset_by_agent = [s, ns, a, r, done]\n",
    "dataset_list = [sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent,sample_dataset_by_agent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf 2.3 and no gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7545114460285562524\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 개별 critic_qnet agent의 그라디언트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "#그래디언트 계산\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        qs = critic_qnet_target_list[0](ns)\n",
    "        sampled_as = get_action(actor_list[0],ns, num_action_samples)\n",
    "        mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "        q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "        # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "        q_val = tf.concat([tf.reshape(tf.gather(critic_qnet_list[0](s)[i],a[i],axis=0),[-1,1]) for i in range(len(s))],axis=0)\n",
    "\n",
    "        loss = loss_fun(q_val, q_target)\n",
    "        # print(loss)\n",
    "\n",
    "        # loss = cal_loss(actor_list[0], critic_qnet_list[0], critic_qnet_target_list[0], s,ns,a,r,done,10,gamma)\n",
    "        # j_loss = joint_loss(actor_list, critic_qnet_list, critic_qnet_target_list,dataset_list,agent_num)\n",
    "        # t.watch(j_loss)\n",
    "        # print([var.name for var in t.watched_variables()])\n",
    "    grads = t.gradient(loss, critic_qnet_list[0].trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, critic_qnet_list[0].trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 에이전트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_list =[]\n",
    "        for j in range(agent_num):\n",
    "            ##데이터 \n",
    "            data = dataset_list[j]\n",
    "            s = data[0]\n",
    "            ns = data[1]\n",
    "            a = data[2]\n",
    "            r = data[3]\n",
    "            done = data[4]\n",
    "            ##에이전트\n",
    "            actor = actor_list[j]\n",
    "            critic_qnet = critic_qnet_list[j]\n",
    "            critic_qnet_target = critic_qnet_target_list[j]\n",
    "            ##로스 계산\n",
    "            qs = critic_qnet_target(ns)\n",
    "            sampled_as = get_action(actor,ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            loss_list.append(loss)\n",
    "        joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "    critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "    grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "    for q in range(agent_num):\n",
    "        optimizer.apply_gradients(zip(grads[q], critic_qnet_weight_list[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 학습 이후 critic_qnet_target 소프트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_list =[]\n",
    "        for j in range(agent_num):\n",
    "            ##데이터 \n",
    "            data = dataset_list[j]\n",
    "            s = data[0]\n",
    "            ns = data[1]\n",
    "            a = data[2]\n",
    "            r = data[3]\n",
    "            done = data[4]\n",
    "            ##에이전트\n",
    "            actor = actor_list[j]\n",
    "            critic_qnet = critic_qnet_list[j]\n",
    "            critic_qnet_target = critic_qnet_target_list[j]\n",
    "            ##로스 계산\n",
    "            qs = critic_qnet_target(ns)\n",
    "            sampled_as = get_action(actor,ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            loss_list.append(loss)\n",
    "        joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "    critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "    critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "    grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "    for q in range(agent_num):\n",
    "        qnet_weights = critic_qnet_weight_list[q]\n",
    "        qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "        grad = grads[q]\n",
    "\n",
    "        optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "\n",
    "        new_target_weights = []\n",
    "        for p, target_weights in enumerate(qnet_target_weights):\n",
    "            qnet_weights_s = qnet_weights[p]\n",
    "            updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "            new_target_weights.append(updated_target_weights_s)\n",
    "        critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss로 target까지 업데이트 후 actor 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss_list =[]\n",
    "        for j in range(agent_num):\n",
    "            ##데이터 \n",
    "            data = dataset_list[j]\n",
    "            s = data[0]\n",
    "            ns = data[1]\n",
    "            a = data[2]\n",
    "            r = data[3]\n",
    "            done = data[4]\n",
    "            ##에이전트\n",
    "            actor = actor_list[j]\n",
    "            critic_qnet = critic_qnet_list[j]\n",
    "            critic_qnet_target = critic_qnet_target_list[j]\n",
    "            ##로스 계산\n",
    "            qs = critic_qnet_target(ns)\n",
    "            sampled_as = get_action(actor,ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            loss_list.append(loss)\n",
    "        joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "    critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "    critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "    grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "    for q in range(agent_num):\n",
    "        # qnet 업데이트\n",
    "        qnet_weights = critic_qnet_weight_list[q]\n",
    "        qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "        grad = grads[q]\n",
    "\n",
    "        optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "        # target net 업데이트\n",
    "        new_target_weights = []\n",
    "        for p, target_weights in enumerate(qnet_target_weights):\n",
    "            qnet_weights_s = qnet_weights[p]\n",
    "            updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "            new_target_weights.append(updated_target_weights_s)\n",
    "        critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n",
    "        #타겟네트워크까지 업데이트한 후 actor net 업데이트\n",
    "        with tf.GradientTape() as tp:\n",
    "            # log_probability 계산\n",
    "            logits = actor_list[q](s)\n",
    "            m = tfp.distributions.Categorical(logits = logits)\n",
    "            log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "            #가중치항 계산\n",
    "            qs = critic_qnet_target_list[q](s)\n",
    "            action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "            vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "            qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "            adv = qas - vs\n",
    "            weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "\n",
    "            #loss\n",
    "            loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "        actor_grad = tp.gradient(loss, actor_list[q].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(actor_grad, actor_list[q].trainable_variables))\n",
    "        # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf 2.8 and RTX 3060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2477650577478363784\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3757100693951550675\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 개별 critic_qnet agent의 그라디언트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.14 s\n",
      "Wall time: 5.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "#그래디언트 계산\n",
    "epoch = 100\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            qs = critic_qnet_target_list[0](ns)\n",
    "            sampled_as = get_action(actor_list[0],ns, num_action_samples)\n",
    "            mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "            q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "            # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "            q_val = tf.concat([tf.reshape(tf.gather(critic_qnet_list[0](s)[i],a[i],axis=0),[-1,1]) for i in range(len(s))],axis=0)\n",
    "\n",
    "            loss = loss_fun(q_val, q_target)\n",
    "            # print(loss)\n",
    "\n",
    "            # loss = cal_loss(actor_list[0], critic_qnet_list[0], critic_qnet_target_list[0], s,ns,a,r,done,10,gamma)\n",
    "            # j_loss = joint_loss(actor_list, critic_qnet_list, critic_qnet_target_list,dataset_list,agent_num)\n",
    "            # t.watch(j_loss)\n",
    "            # print([var.name for var in t.watched_variables()])\n",
    "        grads = t.gradient(loss, critic_qnet_list[0].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, critic_qnet_list[0].trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 에이전트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.3 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    for i in range(100):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            optimizer.apply_gradients(zip(grads[q], critic_qnet_weight_list[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss를 통한 모든 critic_qnet 학습 이후 critic_qnet_target 소프트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21.7 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            qnet_weights = critic_qnet_weight_list[q]\n",
    "            qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "            grad = grads[q]\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "\n",
    "            new_target_weights = []\n",
    "            for p, target_weights in enumerate(qnet_target_weights):\n",
    "                qnet_weights_s = qnet_weights[p]\n",
    "                updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "                new_target_weights.append(updated_target_weights_s)\n",
    "            critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joint loss로 target까지 업데이트 후 actor 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.7 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_list, critic_qnet_list, critic_qnet_target_list = build_structure(agent_num, hidden_structure,input_shape,act_function,output_shape)\n",
    "epoch = 100\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as t:\n",
    "            loss_list =[]\n",
    "            for j in range(agent_num):\n",
    "                ##데이터 \n",
    "                data = dataset_list[j]\n",
    "                s = data[0]\n",
    "                ns = data[1]\n",
    "                a = data[2]\n",
    "                r = data[3]\n",
    "                done = data[4]\n",
    "                ##에이전트\n",
    "                actor = actor_list[j]\n",
    "                critic_qnet = critic_qnet_list[j]\n",
    "                critic_qnet_target = critic_qnet_target_list[j]\n",
    "                ##로스 계산\n",
    "                qs = critic_qnet_target(ns)\n",
    "                sampled_as = get_action(actor,ns, num_action_samples)\n",
    "                mean_qsa = get_mean_qsa(qs,sampled_as)\n",
    "                q_target = r + gamma * mean_qsa * (1 - done)\n",
    "\n",
    "                # 식 3에서 봤던 것처럼 mse형태로 로스함수 설정\n",
    "                q_val = tf.concat([tf.reshape(tf.gather(critic_qnet(s)[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                loss = loss_fun(q_val, q_target)\n",
    "                loss_list.append(loss)\n",
    "            joint_loss = tf.math.reduce_mean(loss_list, axis=None, keepdims=False, name=None)\n",
    "\n",
    "        critic_qnet_weight_list = [critic_qnet_list[p].trainable_variables for p in range(agent_num)]\n",
    "        critic_qnet_target_weight_list = [critic_qnet_target_list[p].trainable_variables for p in range(agent_num)]\n",
    "        grads = t.gradient(joint_loss, critic_qnet_weight_list)\n",
    "\n",
    "        for q in range(agent_num):\n",
    "            # qnet 업데이트\n",
    "            qnet_weights = critic_qnet_weight_list[q]\n",
    "            qnet_target_weights = critic_qnet_target_weight_list[q]\n",
    "            grad = grads[q]\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad, qnet_weights))\n",
    "            # target net 업데이트\n",
    "            new_target_weights = []\n",
    "            for p, target_weights in enumerate(qnet_target_weights):\n",
    "                qnet_weights_s = qnet_weights[p]\n",
    "                updated_target_weights_s = target_weights*(1 - tau) + qnet_weights_s*tau\n",
    "                new_target_weights.append(updated_target_weights_s)\n",
    "            critic_qnet_target_list[q].set_weights(new_target_weights)\n",
    "\n",
    "            #타겟네트워크까지 업데이트한 후 actor net 업데이트\n",
    "            with tf.GradientTape() as tp:\n",
    "                # log_probability 계산\n",
    "                logits = actor_list[q](s)\n",
    "                m = tfp.distributions.Categorical(logits = logits)\n",
    "                log_prob = tf.reshape(m.log_prob(a.squeeze()),[-1,1])\n",
    "\n",
    "                #가중치항 계산\n",
    "                qs = critic_qnet_target_list[q](s)\n",
    "                action_probs = tf.nn.softmax(logits, axis=None, name=None)\n",
    "                vs = tf.math.reduce_sum((qs * action_probs),axis=1, keepdims=True, name=None)\n",
    "                qas = tf.concat([tf.reshape(tf.gather(qs[k],a[k],axis=0),[-1,1]) for k in range(len(s))],axis=0)\n",
    "                adv = qas - vs\n",
    "                weight_term = tf.math.exp((1/lam*adv), name=None)\n",
    "\n",
    "                #loss\n",
    "                loss = tf.math.reduce_mean(log_prob * weight_term*-1)\n",
    "            actor_grad = tp.gradient(loss, actor_list[q].trainable_variables)\n",
    "            optimizer.apply_gradients(zip(actor_grad, actor_list[q].trainable_variables))\n",
    "            # print(loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pro1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
